***************
*** 22,34 ****
  #include <linux/memblock.h>
  #include <linux/slab.h>
  #include <linux/iommu.h>
  #include <linux/vmalloc.h>
  
  #include <asm/memory.h>
  #include <asm/highmem.h>
  #include <asm/cacheflush.h>
  #include <asm/tlbflush.h>
- #include <asm/sizes.h>
  #include <asm/mach/arch.h>
  #include <asm/dma-iommu.h>
  #include <asm/mach/map.h>
--- 22,35 ----
  #include <linux/memblock.h>
  #include <linux/slab.h>
  #include <linux/iommu.h>
+ #include <linux/io.h>
  #include <linux/vmalloc.h>
+ #include <linux/sizes.h>
  
  #include <asm/memory.h>
  #include <asm/highmem.h>
  #include <asm/cacheflush.h>
  #include <asm/tlbflush.h>
  #include <asm/mach/arch.h>
  #include <asm/dma-iommu.h>
  #include <asm/mach/map.h>
***************
*** 72,78 ****
  	     unsigned long offset, size_t size, enum dma_data_direction dir,
  	     struct dma_attrs *attrs)
  {
- 	if (!arch_is_coherent())
  		__dma_page_cpu_to_dev(page, offset, size, dir);
  	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
  }
--- 73,79 ----
  	     unsigned long offset, size_t size, enum dma_data_direction dir,
  	     struct dma_attrs *attrs)
  {
+ 	if (!arch_is_coherent() && !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
  		__dma_page_cpu_to_dev(page, offset, size, dir);
  	return pfn_to_dma(dev, page_to_pfn(page)) + offset;
  }
***************
*** 95,101 ****
  		size_t size, enum dma_data_direction dir,
  		struct dma_attrs *attrs)
  {
- 	if (!arch_is_coherent())
  		__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),
  				      handle & ~PAGE_MASK, size, dir);
  }
--- 96,102 ----
  		size_t size, enum dma_data_direction dir,
  		struct dma_attrs *attrs)
  {
+ 	if (!arch_is_coherent() && !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
  		__dma_page_dev_to_cpu(pfn_to_page(dma_to_pfn(dev, handle)),
  				      handle & ~PAGE_MASK, size, dir);
  }
***************
*** 124,129 ****
  	.alloc			= arm_dma_alloc,
  	.free			= arm_dma_free,
  	.mmap			= arm_dma_mmap,
  	.map_page		= arm_dma_map_page,
  	.unmap_page		= arm_dma_unmap_page,
  	.map_sg			= arm_dma_map_sg,
--- 125,131 ----
  	.alloc			= arm_dma_alloc,
  	.free			= arm_dma_free,
  	.mmap			= arm_dma_mmap,
+ 	.get_sgtable		= arm_dma_get_sgtable,
  	.map_page		= arm_dma_map_page,
  	.unmap_page		= arm_dma_unmap_page,
  	.map_sg			= arm_dma_map_sg,
***************
*** 217,364 ****
  }
  
  #ifdef CONFIG_MMU
  
- #define CONSISTENT_OFFSET(x)	(((unsigned long)(x) - consistent_base) >> PAGE_SHIFT)
- #define CONSISTENT_PTE_INDEX(x) (((unsigned long)(x) - consistent_base) >> PMD_SHIFT)
- 
- /*
-  * These are the page tables (2MB each) covering uncached, DMA consistent allocations
-  */
- static pte_t **consistent_pte;
- 
- #define DEFAULT_CONSISTENT_DMA_SIZE SZ_2M
  
- static unsigned long consistent_base = CONSISTENT_END - DEFAULT_CONSISTENT_DMA_SIZE;
  
- void __init init_consistent_dma_size(unsigned long size)
  {
- 	unsigned long base = CONSISTENT_END - ALIGN(size, SZ_2M);
  
- 	BUG_ON(consistent_pte); /* Check we're called before DMA region init */
- 	BUG_ON(base < VMALLOC_END);
  
- 	/* Grow region to accommodate specified size  */
- 	if (base < consistent_base)
- 		consistent_base = base;
  }
  
- #include "vmregion.h"
- 
- static struct arm_vmregion_head consistent_head = {
- 	.vm_lock	= __SPIN_LOCK_UNLOCKED(&consistent_head.vm_lock),
- 	.vm_list	= LIST_HEAD_INIT(consistent_head.vm_list),
- 	.vm_end		= CONSISTENT_END,
- };
- 
- #ifdef CONFIG_HUGETLB_PAGE
- #error ARM Coherent DMA allocator does not (yet) support huge TLB
- #endif
- 
- /*
-  * Initialise the consistent memory allocation.
-  */
- static int __init consistent_init(void)
  {
- 	int ret = 0;
- 	pgd_t *pgd;
- 	pud_t *pud;
- 	pmd_t *pmd;
- 	pte_t *pte;
- 	int i = 0;
- 	unsigned long base = consistent_base;
- 	unsigned long num_ptes = (CONSISTENT_END - base) >> PMD_SHIFT;
- 
- 	if (IS_ENABLED(CONFIG_CMA) && !IS_ENABLED(CONFIG_ARM_DMA_USE_IOMMU))
- 		return 0;
- 
- 	consistent_pte = kmalloc(num_ptes * sizeof(pte_t), GFP_KERNEL);
- 	if (!consistent_pte) {
- 		pr_err("%s: no memory\n", __func__);
- 		return -ENOMEM;
  	}
- 
- 	pr_debug("DMA memory: 0x%08lx - 0x%08lx:\n", base, CONSISTENT_END);
- 	consistent_head.vm_start = base;
- 
- 	do {
- 		pgd = pgd_offset(&init_mm, base);
- 
- 		pud = pud_alloc(&init_mm, pgd, base);
- 		if (!pud) {
- 			pr_err("%s: no pud tables\n", __func__);
- 			ret = -ENOMEM;
- 			break;
- 		}
- 
- 		pmd = pmd_alloc(&init_mm, pud, base);
- 		if (!pmd) {
- 			pr_err("%s: no pmd tables\n", __func__);
- 			ret = -ENOMEM;
- 			break;
- 		}
- 		WARN_ON(!pmd_none(*pmd));
- 
- 		pte = pte_alloc_kernel(pmd, base);
- 		if (!pte) {
- 			pr_err("%s: no pte tables\n", __func__);
- 			ret = -ENOMEM;
- 			break;
- 		}
- 
- 		consistent_pte[i++] = pte;
- 		base += PMD_SIZE;
- 	} while (base < CONSISTENT_END);
- 
- 	return ret;
  }
- core_initcall(consistent_init);
  
- static void *__alloc_from_contiguous(struct device *dev, size_t size,
- 				     pgprot_t prot, struct page **ret_page);
  
- static struct arm_vmregion_head coherent_head = {
- 	.vm_lock	= __SPIN_LOCK_UNLOCKED(&coherent_head.vm_lock),
- 	.vm_list	= LIST_HEAD_INIT(coherent_head.vm_list),
  };
  
- static size_t coherent_pool_size = DEFAULT_CONSISTENT_DMA_SIZE / 8;
  
  static int __init early_coherent_pool(char *p)
  {
- 	coherent_pool_size = memparse(p, &p);
  	return 0;
  }
  early_param("coherent_pool", early_coherent_pool);
  
  /*
   * Initialise the coherent pool for atomic allocations.
   */
- static int __init coherent_init(void)
  {
  	pgprot_t prot = pgprot_dmacoherent(pgprot_kernel);
- 	size_t size = coherent_pool_size;
  	struct page *page;
  	void *ptr;
  
- 	if (!IS_ENABLED(CONFIG_CMA))
- 		return 0;
  
- 	ptr = __alloc_from_contiguous(NULL, size, prot, &page);
  	if (ptr) {
- 		coherent_head.vm_start = (unsigned long) ptr;
- 		coherent_head.vm_end = (unsigned long) ptr + size;
- 		printk(KERN_INFO "DMA: preallocated %u KiB pool for atomic coherent allocations\n",
- 		       (unsigned)size / 1024);
  		return 0;
  	}
- 	printk(KERN_ERR "DMA: failed to allocate %u KiB pool for atomic coherent allocation\n",
- 	       (unsigned)size / 1024);
  	return -ENOMEM;
  }
  /*
   * CMA is activated by core_initcall, so we must be called after it.
   */
- postcore_initcall(coherent_init);
  
  struct dma_contig_early_reserve {
  	phys_addr_t base;
--- 219,364 ----
  }
  
  #ifdef CONFIG_MMU
+ #ifdef CONFIG_HUGETLB_PAGE
+ #error ARM Coherent DMA allocator does not (yet) support huge TLB
+ #endif
  
+ static void *__alloc_from_contiguous(struct device *dev, size_t size,
+ 				     pgprot_t prot, struct page **ret_page);
  
+ static void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,
+ 				 pgprot_t prot, struct page **ret_page,
+ 				 const void *caller);
  
+ static void *
+ __dma_alloc_remap(struct page *page, size_t size, gfp_t gfp, pgprot_t prot,
+ 	const void *caller)
  {
+ 	struct vm_struct *area;
+ 	unsigned long addr;
  
+ 	/*
+ 	 * DMA allocation can be mapped to user space, so lets
+ 	 * set VM_USERMAP flags too.
+ 	 */
+ 	area = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,
+ 				  caller);
+ 	if (!area)
+ 		return NULL;
+ 	addr = (unsigned long)area->addr;
+ 	area->phys_addr = __pfn_to_phys(page_to_pfn(page));
  
+ 	if (ioremap_page_range(addr, addr + size, area->phys_addr, prot)) {
+ 		vunmap((void *)addr);
+ 		return NULL;
+ 	}
+ 	return (void *)addr;
  }
  
+ static void __dma_free_remap(void *cpu_addr, size_t size)
  {
+ 	unsigned int flags = VM_ARM_DMA_CONSISTENT | VM_USERMAP;
+ 	struct vm_struct *area = find_vm_area(cpu_addr);
+ 	if (!area || (area->flags & flags) != flags) {
+ 		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
+ 		return;
  	}
+ 	unmap_kernel_range((unsigned long)cpu_addr, size);
+ 	vunmap(cpu_addr);
  }
  
+ #define DEFAULT_DMA_COHERENT_POOL_SIZE	SZ_256K
  
+ struct dma_pool {
+ 	size_t size;
+ 	spinlock_t lock;
+ 	unsigned long *bitmap;
+ 	unsigned long nr_pages;
+ 	void *vaddr;
+ 	struct page **pages;
  };
  
+ static struct dma_pool atomic_pool = {
+ 	.size = DEFAULT_DMA_COHERENT_POOL_SIZE,
+ };
  
  static int __init early_coherent_pool(char *p)
  {
+ 	atomic_pool.size = memparse(p, &p);
  	return 0;
  }
  early_param("coherent_pool", early_coherent_pool);
  
+ void __init init_dma_coherent_pool_size(unsigned long size)
+ {
+ 	/*
+ 	 * Catch any attempt to set the pool size too late.
+ 	 */
+ 	BUG_ON(atomic_pool.vaddr);
+ 
+ 	/*
+ 	 * Set architecture specific coherent pool size only if
+ 	 * it has not been changed by kernel command line parameter.
+ 	 */
+ 	if (atomic_pool.size == DEFAULT_DMA_COHERENT_POOL_SIZE)
+ 		atomic_pool.size = size;
+ }
+ 
  /*
   * Initialise the coherent pool for atomic allocations.
   */
+ static int __init atomic_pool_init(void)
  {
+ 	struct dma_pool *pool = &atomic_pool;
  	pgprot_t prot = pgprot_dmacoherent(pgprot_kernel);
+ 	unsigned long nr_pages = pool->size >> PAGE_SHIFT;
+ 	unsigned long *bitmap;
  	struct page *page;
+ 	struct page **pages;
  	void *ptr;
+ 	int bitmap_size = BITS_TO_LONGS(nr_pages) * sizeof(long);
  
+ 	bitmap = kzalloc(bitmap_size, GFP_KERNEL);
+ 	if (!bitmap)
+ 		goto no_bitmap;
+ 
+ 	pages = kzalloc(nr_pages * sizeof(struct page *), GFP_KERNEL);
+ 	if (!pages)
+ 		goto no_pages;
  
+ 	if (IS_ENABLED(CONFIG_CMA))
+ 		ptr = __alloc_from_contiguous(NULL, pool->size, prot, &page);
+ 	else
+ 		ptr = __alloc_remap_buffer(NULL, pool->size, GFP_KERNEL, prot,
+ 					   &page, NULL);
  	if (ptr) {
+ 		int i;
+ 
+ 		for (i = 0; i < nr_pages; i++)
+ 			pages[i] = page + i;
+ 
+ 		spin_lock_init(&pool->lock);
+ 		pool->vaddr = ptr;
+ 		pool->pages = pages;
+ 		pool->bitmap = bitmap;
+ 		pool->nr_pages = nr_pages;
+ 		pr_info("DMA: preallocated %u KiB pool for atomic coherent allocations\n",
+ 		       (unsigned)pool->size / 1024);
  		return 0;
  	}
+ 
+ 	kfree(pages);
+ no_pages:
+ 	kfree(bitmap);
+ no_bitmap:
+ 	pr_err("DMA: failed to allocate %u KiB pool for atomic coherent allocation\n",
+ 	       (unsigned)pool->size / 1024);
  	return -ENOMEM;
  }
  /*
   * CMA is activated by core_initcall, so we must be called after it.
   */
+ postcore_initcall(atomic_pool_init);
  
  struct dma_contig_early_reserve {
  	phys_addr_t base;
***************
*** 372,483 ****
  	}
  }
  
- static void *
- __dma_alloc_remap(struct page *page, size_t size, gfp_t gfp, pgprot_t prot,
- 	const void *caller)
- {
- 	struct arm_vmregion *c;
- 	size_t align;
- 	int bit;
- 
- 	if (!consistent_pte) {
- 		pr_err("%s: not initialised\n", __func__);
- 		dump_stack();
- 		return NULL;
- 	}
- 
- 	/*
- 	 * Align the virtual region allocation - maximum alignment is
- 	 * a section size, minimum is a page size.  This helps reduce
- 	 * fragmentation of the DMA space, and also prevents allocations
- 	 * smaller than a section from crossing a section boundary.
- 	 */
- 	bit = fls(size - 1);
- 	if (bit > SECTION_SHIFT)
- 		bit = SECTION_SHIFT;
- 	align = 1 << bit;
- 
- 	/*
- 	 * Allocate a virtual address in the consistent mapping region.
- 	 */
- 	c = arm_vmregion_alloc(&consistent_head, align, size,
- 			    gfp & ~(__GFP_DMA | __GFP_HIGHMEM), caller);
- 	if (c) {
- 		pte_t *pte;
- 		int idx = CONSISTENT_PTE_INDEX(c->vm_start);
- 		u32 off = CONSISTENT_OFFSET(c->vm_start) & (PTRS_PER_PTE-1);
- 
- 		pte = consistent_pte[idx] + off;
- 		c->priv = page;
- 
- 		do {
- 			BUG_ON(!pte_none(*pte));
- 
- 			set_pte_ext(pte, mk_pte(page, prot), 0);
- 			page++;
- 			pte++;
- 			off++;
- 			if (off >= PTRS_PER_PTE) {
- 				off = 0;
- 				pte = consistent_pte[++idx];
- 			}
- 		} while (size -= PAGE_SIZE);
- 
- 		dsb();
- 
- 		return (void *)c->vm_start;
- 	}
- 	return NULL;
- }
- 
- static void __dma_free_remap(void *cpu_addr, size_t size)
- {
- 	struct arm_vmregion *c;
- 	unsigned long addr;
- 	pte_t *ptep;
- 	int idx;
- 	u32 off;
- 
- 	c = arm_vmregion_find_remove(&consistent_head, (unsigned long)cpu_addr);
- 	if (!c) {
- 		pr_err("%s: trying to free invalid coherent area: %p\n",
- 		       __func__, cpu_addr);
- 		dump_stack();
- 		return;
- 	}
- 
- 	if ((c->vm_end - c->vm_start) != size) {
- 		pr_err("%s: freeing wrong coherent size (%ld != %d)\n",
- 		       __func__, c->vm_end - c->vm_start, size);
- 		dump_stack();
- 		size = c->vm_end - c->vm_start;
- 	}
- 
- 	idx = CONSISTENT_PTE_INDEX(c->vm_start);
- 	off = CONSISTENT_OFFSET(c->vm_start) & (PTRS_PER_PTE-1);
- 	ptep = consistent_pte[idx] + off;
- 	addr = c->vm_start;
- 	do {
- 		pte_t pte = ptep_get_and_clear(&init_mm, addr, ptep);
- 
- 		ptep++;
- 		addr += PAGE_SIZE;
- 		off++;
- 		if (off >= PTRS_PER_PTE) {
- 			off = 0;
- 			ptep = consistent_pte[++idx];
- 		}
- 
- 		if (pte_none(pte) || !pte_present(pte))
- 			pr_crit("%s: bad page in kernel page table\n",
- 				__func__);
- 	} while (size -= PAGE_SIZE);
- 
- 	flush_tlb_kernel_range(c->vm_start, c->vm_end);
- 
- 	arm_vmregion_free(&consistent_head, c);
- }
- 
  static int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr,
  			    void *data)
  {
--- 372,377 ----
  	}
  }
  
  static int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr,
  			    void *data)
  {
***************
*** 518,533 ****
  	return ptr;
  }
  
- static void *__alloc_from_pool(struct device *dev, size_t size,
- 			       struct page **ret_page, const void *caller)
  {
- 	struct arm_vmregion *c;
- 	size_t align;
  
- 	if (!coherent_head.vm_start) {
- 		printk(KERN_ERR "%s: coherent pool not initialised!\n",
- 		       __func__);
- 		dump_stack();
  		return NULL;
  	}
  
--- 412,428 ----
  	return ptr;
  }
  
+ static void *__alloc_from_pool(size_t size, struct page **ret_page)
  {
+ 	struct dma_pool *pool = &atomic_pool;
+ 	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	unsigned int pageno;
+ 	unsigned long flags;
+ 	void *ptr = NULL;
+ 	unsigned long align_mask;
  
+ 	if (!pool->vaddr) {
+ 		WARN(1, "coherent pool not initialised!\n");
  		return NULL;
  	}
  
***************
*** 536,571 ****
  	 * small, so align them to their order in pages, minimum is a page
  	 * size. This helps reduce fragmentation of the DMA space.
  	 */
- 	align = PAGE_SIZE << get_order(size);
- 	c = arm_vmregion_alloc(&coherent_head, align, size, 0, caller);
- 	if (c) {
- 		void *ptr = (void *)c->vm_start;
- 		struct page *page = virt_to_page(ptr);
- 		*ret_page = page;
- 		return ptr;
  	}
- 	return NULL;
  }
  
- static int __free_from_pool(void *cpu_addr, size_t size)
  {
- 	unsigned long start = (unsigned long)cpu_addr;
- 	unsigned long end = start + size;
- 	struct arm_vmregion *c;
  
- 	if (start < coherent_head.vm_start || end > coherent_head.vm_end)
  		return 0;
  
- 	c = arm_vmregion_find_remove(&coherent_head, (unsigned long)start);
  
- 	if ((c->vm_end - c->vm_start) != size) {
- 		printk(KERN_ERR "%s: freeing wrong coherent size (%ld != %d)\n",
- 		       __func__, c->vm_end - c->vm_start, size);
- 		dump_stack();
- 		size = c->vm_end - c->vm_start;
- 	}
  
- 	arm_vmregion_free(&coherent_head, c);
  	return 1;
  }
  
--- 431,490 ----
  	 * small, so align them to their order in pages, minimum is a page
  	 * size. This helps reduce fragmentation of the DMA space.
  	 */
+ 	align_mask = (1 << get_order(size)) - 1;
+ 
+ 	spin_lock_irqsave(&pool->lock, flags);
+ 	pageno = bitmap_find_next_zero_area(pool->bitmap, pool->nr_pages,
+ 					    0, count, align_mask);
+ 	if (pageno < pool->nr_pages) {
+ 		bitmap_set(pool->bitmap, pageno, count);
+ 		ptr = pool->vaddr + PAGE_SIZE * pageno;
+ 		*ret_page = pool->pages[pageno];
+ 	} else {
+ 		pr_err_once("ERROR: %u KiB atomic DMA coherent pool is too small!\n"
+ 			    "Please increase it with coherent_pool= kernel parameter!\n",
+ 			    (unsigned)pool->size / 1024);
  	}
+ 	spin_unlock_irqrestore(&pool->lock, flags);
+ 
+ 	return ptr;
+ }
+ 
+ static bool __in_atomic_pool(void *start, size_t size)
+ {
+ 	struct dma_pool *pool = &atomic_pool;
+ 	void *end = start + size;
+ 	void *pool_start = pool->vaddr;
+ 	void *pool_end = pool->vaddr + pool->size;
+ 
+ 	if (start < pool_start || start >= pool_end)
+ 		return false;
+ 
+ 	if (end <= pool_end)
+ 		return true;
+ 
+ 	WARN(1, "Wrong coherent size(%p-%p) from atomic pool(%p-%p)\n",
+ 	     start, end - 1, pool_start, pool_end - 1);
+ 
+ 	return false;
  }
  
+ static int __free_from_pool(void *start, size_t size)
  {
+ 	struct dma_pool *pool = &atomic_pool;
+ 	unsigned long pageno, count;
+ 	unsigned long flags;
  
+ 	if (!__in_atomic_pool(start, size))
  		return 0;
  
+ 	pageno = (start - pool->vaddr) >> PAGE_SHIFT;
+ 	count = size >> PAGE_SHIFT;
  
+ 	spin_lock_irqsave(&pool->lock, flags);
+ 	bitmap_clear(pool->bitmap, pageno, count);
+ 	spin_unlock_irqrestore(&pool->lock, flags);
  
  	return 1;
  }
  
***************
*** 571,580 ****
  
  	if (arch_is_coherent() || nommu())
  		addr = __alloc_simple_buffer(dev, size, gfp, &page);
  	else if (!IS_ENABLED(CONFIG_CMA))
  		addr = __alloc_remap_buffer(dev, size, gfp, prot, &page, caller);
- 	else if (gfp & GFP_ATOMIC)
- 		addr = __alloc_from_pool(dev, size, &page, caller);
  	else
  		addr = __alloc_from_contiguous(dev, size, prot, &page);
  
--- 490,499 ----
  
  	if (arch_is_coherent() || nommu())
  		addr = __alloc_simple_buffer(dev, size, gfp, &page);
+ 	else if (gfp & GFP_ATOMIC)
+ 		addr = __alloc_from_pool(size, &page);
  	else if (!IS_ENABLED(CONFIG_CMA))
  		addr = __alloc_remap_buffer(dev, size, gfp, prot, &page, caller);
  	else
  		addr = __alloc_from_contiguous(dev, size, prot, &page);
  
***************
*** 610,625 ****
  {
  	int ret = -ENXIO;
  #ifdef CONFIG_MMU
  	unsigned long pfn = dma_to_pfn(dev, dma_addr);
  	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);
  
  	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))
  		return ret;
  
- 	ret = remap_pfn_range(vma, vma->vm_start,
- 			      pfn + vma->vm_pgoff,
- 			      vma->vm_end - vma->vm_start,
- 			      vma->vm_page_prot);
  #endif	/* CONFIG_MMU */
  
  	return ret;
--- 529,550 ----
  {
  	int ret = -ENXIO;
  #ifdef CONFIG_MMU
+ 	unsigned long nr_vma_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+ 	unsigned long nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;
  	unsigned long pfn = dma_to_pfn(dev, dma_addr);
+ 	unsigned long off = vma->vm_pgoff;
+ 
  	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);
  
  	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &ret))
  		return ret;
  
+ 	if (off < nr_pages && nr_vma_pages <= (nr_pages - off)) {
+ 		ret = remap_pfn_range(vma, vma->vm_start,
+ 				      pfn + off,
+ 				      vma->vm_end - vma->vm_start,
+ 				      vma->vm_page_prot);
+ 	}
  #endif	/* CONFIG_MMU */
  
  	return ret;
***************
*** 640,651 ****
  
  	if (arch_is_coherent() || nommu()) {
  		__dma_free_buffer(page, size);
  	} else if (!IS_ENABLED(CONFIG_CMA)) {
  		__dma_free_remap(cpu_addr, size);
  		__dma_free_buffer(page, size);
  	} else {
- 		if (__free_from_pool(cpu_addr, size))
- 			return;
  		/*
  		 * Non-atomic allocations cannot be freed with IRQs disabled
  		 */
--- 565,576 ----
  
  	if (arch_is_coherent() || nommu()) {
  		__dma_free_buffer(page, size);
+ 	} else if (__free_from_pool(cpu_addr, size)) {
+ 		return;
  	} else if (!IS_ENABLED(CONFIG_CMA)) {
  		__dma_free_remap(cpu_addr, size);
  		__dma_free_buffer(page, size);
  	} else {
  		/*
  		 * Non-atomic allocations cannot be freed with IRQs disabled
  		 */
***************
*** 888,896 ****
  
  static int __init dma_debug_do_init(void)
  {
- #ifdef CONFIG_MMU
- 	arm_vmregion_create_proc("dma-mappings", &consistent_head);
- #endif
  	dma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);
  	return 0;
  }
--- 828,833 ----
  
  static int __init dma_debug_do_init(void)
  {
  	dma_debug_init(PREALLOC_DMA_DEBUG_ENTRIES);
  	return 0;
  }
***************
*** 1004,1064 ****
   * Create a CPU mapping for a specified pages
   */
  static void *
- __iommu_alloc_remap(struct page **pages, size_t size, gfp_t gfp, pgprot_t prot)
  {
- 	struct arm_vmregion *c;
- 	size_t align;
- 	size_t count = size >> PAGE_SHIFT;
- 	int bit;
  
- 	if (!consistent_pte[0]) {
- 		pr_err("%s: not initialised\n", __func__);
- 		dump_stack();
  		return NULL;
- 	}
  
- 	/*
- 	 * Align the virtual region allocation - maximum alignment is
- 	 * a section size, minimum is a page size.  This helps reduce
- 	 * fragmentation of the DMA space, and also prevents allocations
- 	 * smaller than a section from crossing a section boundary.
- 	 */
- 	bit = fls(size - 1);
- 	if (bit > SECTION_SHIFT)
- 		bit = SECTION_SHIFT;
- 	align = 1 << bit;
  
- 	/*
- 	 * Allocate a virtual address in the consistent mapping region.
- 	 */
- 	c = arm_vmregion_alloc(&consistent_head, align, size,
- 			    gfp & ~(__GFP_DMA | __GFP_HIGHMEM), NULL);
- 	if (c) {
- 		pte_t *pte;
- 		int idx = CONSISTENT_PTE_INDEX(c->vm_start);
- 		int i = 0;
- 		u32 off = CONSISTENT_OFFSET(c->vm_start) & (PTRS_PER_PTE-1);
- 
- 		pte = consistent_pte[idx] + off;
- 		c->priv = pages;
- 
- 		do {
- 			BUG_ON(!pte_none(*pte));
- 
- 			set_pte_ext(pte, mk_pte(pages[i], prot), 0);
- 			pte++;
- 			off++;
- 			i++;
- 			if (off >= PTRS_PER_PTE) {
- 				off = 0;
- 				pte = consistent_pte[++idx];
- 			}
- 		} while (i < count);
- 
- 		dsb();
- 
- 		return (void *)c->vm_start;
  	}
  	return NULL;
  }
  
--- 941,972 ----
   * Create a CPU mapping for a specified pages
   */
  static void *
+ __iommu_alloc_remap(struct page **pages, size_t size, gfp_t gfp, pgprot_t prot,
+ 		    const void *caller)
  {
+ 	unsigned int i, nr_pages = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	struct vm_struct *area;
+ 	unsigned long p;
  
+ 	area = get_vm_area_caller(size, VM_ARM_DMA_CONSISTENT | VM_USERMAP,
+ 				  caller);
+ 	if (!area)
  		return NULL;
  
+ 	area->pages = pages;
+ 	area->nr_pages = nr_pages;
+ 	p = (unsigned long)area->addr;
  
+ 	for (i = 0; i < nr_pages; i++) {
+ 		phys_addr_t phys = __pfn_to_phys(page_to_pfn(pages[i]));
+ 		if (ioremap_page_range(p, p + PAGE_SIZE, phys, prot))
+ 			goto err;
+ 		p += PAGE_SIZE;
  	}
+ 	return area->addr;
+ err:
+ 	unmap_kernel_range((unsigned long)area->addr, size);
+ 	vunmap(area->addr);
  	return NULL;
  }
  
***************
*** 1118,1124 ****
  	if (*handle == DMA_ERROR_CODE)
  		goto err_buffer;
  
- 	addr = __iommu_alloc_remap(pages, size, gfp, prot);
  	if (!addr)
  		goto err_mapping;
  
--- 1082,1092 ----
  	if (*handle == DMA_ERROR_CODE)
  		goto err_buffer;
  
+ 	if (dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs))
+ 		return pages;
+ 
+ 	addr = __iommu_alloc_remap(pages, size, gfp, prot,
+ 				   __builtin_return_address(0));
  	if (!addr)
  		goto err_mapping;
  
***************
*** 1135,1165 ****
  		    void *cpu_addr, dma_addr_t dma_addr, size_t size,
  		    struct dma_attrs *attrs)
  {
- 	struct arm_vmregion *c;
  
  	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);
- 	c = arm_vmregion_find(&consistent_head, (unsigned long)cpu_addr);
- 
- 	if (c) {
- 		struct page **pages = c->priv;
- 
- 		unsigned long uaddr = vma->vm_start;
- 		unsigned long usize = vma->vm_end - vma->vm_start;
- 		int i = 0;
  
- 		do {
- 			int ret;
  
- 			ret = vm_insert_page(vma, uaddr, pages[i++]);
- 			if (ret) {
- 				pr_err("Remapping memory, error: %d\n", ret);
- 				return ret;
- 			}
  
- 			uaddr += PAGE_SIZE;
- 			usize -= PAGE_SIZE;
- 		} while (usize > 0);
- 	}
  	return 0;
  }
  
--- 1103,1127 ----
  		    void *cpu_addr, dma_addr_t dma_addr, size_t size,
  		    struct dma_attrs *attrs)
  {
+ 	unsigned long uaddr = vma->vm_start;
+ 	unsigned long usize = vma->vm_end - vma->vm_start;
+ 	struct page **pages = __iommu_get_pages(cpu_addr, attrs);
  
  	vma->vm_page_prot = __get_dma_pgprot(attrs, vma->vm_page_prot);
  
+ 	if (!pages)
+ 		return -ENXIO;
  
+ 	do {
+ 		int ret = vm_insert_page(vma, uaddr, *pages++);
+ 		if (ret) {
+ 			pr_err("Remapping memory failed: %d\n", ret);
+ 			return ret;
+ 		}
+ 		uaddr += PAGE_SIZE;
+ 		usize -= PAGE_SIZE;
+ 	} while (usize > 0);
  
  	return 0;
  }
  
***************
*** 1170,1185 ****
  void arm_iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,
  			  dma_addr_t handle, struct dma_attrs *attrs)
  {
- 	struct arm_vmregion *c;
  	size = PAGE_ALIGN(size);
  
- 	c = arm_vmregion_find(&consistent_head, (unsigned long)cpu_addr);
- 	if (c) {
- 		struct page **pages = c->priv;
- 		__dma_free_remap(cpu_addr, size);
- 		__iommu_remove_mapping(dev, handle, size);
- 		__iommu_free_buffer(dev, pages, size);
  	}
  }
  
  /*
--- 1132,1171 ----
  void arm_iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,
  			  dma_addr_t handle, struct dma_attrs *attrs)
  {
+ 	struct page **pages = __iommu_get_pages(cpu_addr, attrs);
  	size = PAGE_ALIGN(size);
  
+ 	if (!pages) {
+ 		WARN(1, "trying to free invalid coherent area: %p\n", cpu_addr);
+ 		return;
  	}
+ 
+ 	if (__in_atomic_pool(cpu_addr, size)) {
+ 		__iommu_free_atomic(dev, pages, handle, size);
+ 		return;
+ 	}
+ 
+ 	if (!dma_get_attr(DMA_ATTR_NO_KERNEL_MAPPING, attrs)) {
+ 		unmap_kernel_range((unsigned long)cpu_addr, size);
+ 		vunmap(cpu_addr);
+ 	}
+ 
+ 	__iommu_remove_mapping(dev, handle, size);
+ 	__iommu_free_buffer(dev, pages, size);
+ }
+ 
+ static int arm_iommu_get_sgtable(struct device *dev, struct sg_table *sgt,
+ 				 void *cpu_addr, dma_addr_t dma_addr,
+ 				 size_t size, struct dma_attrs *attrs)
+ {
+ 	unsigned int count = PAGE_ALIGN(size) >> PAGE_SHIFT;
+ 	struct page **pages = __iommu_get_pages(cpu_addr, attrs);
+ 
+ 	if (!pages)
+ 		return -ENXIO;
+ 
+ 	return sg_alloc_table_from_pages(sgt, pages, count, 0, size,
+ 					 GFP_KERNEL);
  }
  
  /*
***************
*** 1187,1193 ****
   */
  static int __map_sg_chunk(struct device *dev, struct scatterlist *sg,
  			  size_t size, dma_addr_t *handle,
- 			  enum dma_data_direction dir)
  {
  	struct dma_iommu_mapping *mapping = dev->archdata.mapping;
  	dma_addr_t iova, iova_base;
--- 1173,1179 ----
   */
  static int __map_sg_chunk(struct device *dev, struct scatterlist *sg,
  			  size_t size, dma_addr_t *handle,
+ 			  enum dma_data_direction dir, struct dma_attrs *attrs)
  {
  	struct dma_iommu_mapping *mapping = dev->archdata.mapping;
  	dma_addr_t iova, iova_base;
***************
*** 1206,1212 ****
  		phys_addr_t phys = page_to_phys(sg_page(s));
  		unsigned int len = PAGE_ALIGN(s->offset + s->length);
  
- 		if (!arch_is_coherent())
  			__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);
  
  		ret = iommu_map(mapping->domain, iova, phys, len, 0);
--- 1192,1199 ----
  		phys_addr_t phys = page_to_phys(sg_page(s));
  		unsigned int len = PAGE_ALIGN(s->offset + s->length);
  
+ 		if (!arch_is_coherent() &&
+ 		    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
  			__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);
  
  		ret = iommu_map(mapping->domain, iova, phys, len, 0);
***************
*** 1253,1259 ****
  
  		if (s->offset || (size & ~PAGE_MASK) || size + s->length > max) {
  			if (__map_sg_chunk(dev, start, size, &dma->dma_address,
- 			    dir) < 0)
  				goto bad_mapping;
  
  			dma->dma_address += offset;
--- 1240,1246 ----
  
  		if (s->offset || (size & ~PAGE_MASK) || size + s->length > max) {
  			if (__map_sg_chunk(dev, start, size, &dma->dma_address,
+ 			    dir, attrs) < 0)
  				goto bad_mapping;
  
  			dma->dma_address += offset;
***************
*** 1266,1272 ****
  		}
  		size += s->length;
  	}
- 	if (__map_sg_chunk(dev, start, size, &dma->dma_address, dir) < 0)
  		goto bad_mapping;
  
  	dma->dma_address += offset;
--- 1253,1259 ----
  		}
  		size += s->length;
  	}
+ 	if (__map_sg_chunk(dev, start, size, &dma->dma_address, dir, attrs) < 0)
  		goto bad_mapping;
  
  	dma->dma_address += offset;
***************
*** 1300,1306 ****
  		if (sg_dma_len(s))
  			__iommu_remove_mapping(dev, sg_dma_address(s),
  					       sg_dma_len(s));
- 		if (!arch_is_coherent())
  			__dma_page_dev_to_cpu(sg_page(s), s->offset,
  					      s->length, dir);
  	}
--- 1287,1294 ----
  		if (sg_dma_len(s))
  			__iommu_remove_mapping(dev, sg_dma_address(s),
  					       sg_dma_len(s));
+ 		if (!arch_is_coherent() &&
+ 		    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
  			__dma_page_dev_to_cpu(sg_page(s), s->offset,
  					      s->length, dir);
  	}
***************
*** 1362,1368 ****
  	dma_addr_t dma_addr;
  	int ret, len = PAGE_ALIGN(size + offset);
  
- 	if (!arch_is_coherent())
  		__dma_page_cpu_to_dev(page, offset, size, dir);
  
  	dma_addr = __alloc_iova(mapping, len);
--- 1350,1356 ----
  	dma_addr_t dma_addr;
  	int ret, len = PAGE_ALIGN(size + offset);
  
+ 	if (!arch_is_coherent() && !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
  		__dma_page_cpu_to_dev(page, offset, size, dir);
  
  	dma_addr = __alloc_iova(mapping, len);
***************
*** 1480,1485 ****
  	.alloc		= arm_iommu_alloc_attrs,
  	.free		= arm_iommu_free_attrs,
  	.mmap		= arm_iommu_mmap_attrs,
  
  	.map_page		= arm_iommu_map_page,
  	.unmap_page		= arm_iommu_unmap_page,
--- 1468,1474 ----
  	.alloc		= arm_iommu_alloc_attrs,
  	.free		= arm_iommu_free_attrs,
  	.mmap		= arm_iommu_mmap_attrs,
+ 	.get_sgtable	= arm_iommu_get_sgtable,
  
  	.map_page		= arm_iommu_map_page,
  	.unmap_page		= arm_iommu_unmap_page,
