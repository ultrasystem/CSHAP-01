***************
*** 26,32 ****
  __mutex_fastpath_lock(atomic_t *count, void (*fail_fn)(atomic_t *))
  {
  	if (unlikely(atomic_xchg(count, 0) != 1))
- 		fail_fn(count);
  }
  
  /**
--- 26,38 ----
  __mutex_fastpath_lock(atomic_t *count, void (*fail_fn)(atomic_t *))
  {
  	if (unlikely(atomic_xchg(count, 0) != 1))
+ 		/*
+ 		 * We failed to acquire the lock, so mark it contended
+ 		 * to ensure that any waiting tasks are woken up by the
+ 		 * unlock slow path.
+ 		 */
+ 		if (likely(atomic_xchg(count, -1) != 1))
+ 			fail_fn(count);
  }
  
  /**
***************
*** 43,49 ****
  __mutex_fastpath_lock_retval(atomic_t *count, int (*fail_fn)(atomic_t *))
  {
  	if (unlikely(atomic_xchg(count, 0) != 1))
- 		return fail_fn(count);
  	return 0;
  }
  
--- 49,56 ----
  __mutex_fastpath_lock_retval(atomic_t *count, int (*fail_fn)(atomic_t *))
  {
  	if (unlikely(atomic_xchg(count, 0) != 1))
+ 		if (likely(atomic_xchg(count, -1) != 1))
+ 			return fail_fn(count);
  	return 0;
  }
  
